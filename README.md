# Customer Churn Prediction Models

This repository contains multiple machine learning models developed to predict **customer churn** using a real-world dataset. Built as part of the **Kodessphere Hackathon 2024**, the project explores different algorithms, evaluates their performance, and draws insights to support customer retention strategies.

---

## üåü Hackathon Project

**üèÜ Team Name**: Nova
**üìÖ Date**: March 2024
**üåê Event**: Kodessphere Hackathon 2024
**üìç Goal**: Predict customer churn with high recall and actionable insights for telecom service providers.

---

## Project Structure

```
.
‚îú‚îÄ‚îÄ dataset/
‚îÇ   ‚îî‚îÄ‚îÄ telecom_churn_data.csv
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ logistic_regression_model.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ decision_tree_model.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ random_forest_model.ipynb
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrices/
‚îÇ   ‚îî‚îÄ‚îÄ classification_reports/
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ assets/
    ‚îî‚îÄ‚îÄ demo-thumbnail.png (optional)
```

---

## Dataset

* Format: CSV
* Records: 2,321 customers
* Features: Includes usage metrics, contract details, payment info, etc.
* Target: `Churn` (binary classification ‚Äì 0: Non-churn, 1: Churn)

---

## Models Used

1. **Logistic Regression**
2. **Decision Tree Classifier**
3. **Random Forest Classifier**

Each model was trained and evaluated using:

* Train/Test split
* Confusion Matrix
* Classification Report (Precision, Recall, F1-score)
* Accuracy Score

---

## Performance Summary (for one algorithm)

| Model               | Accuracy | Recall (Churn) | Precision (Churn) | F1-score (Churn) |
| ------------------- | -------- | -------------- | ----------------- | ---------------- |
| Logistic Regression | \~63%    | \~62%          | \~38%             | \~47%            |
| Decision Tree       | \~67%    | \~60%          | \~41%             | \~49%            |
| Random Forest       | **65%**  | **66%**        | **40%**           | **49%**          |

> The Random Forest model achieved the best balance between identifying churners and minimizing false positives.

---

## Confusion Matrix (Random Forest)

```
[[1102  609]
 [ 210  400]]
```

## Conclusion

* The models offer moderate accuracy but strong **recall for churners**, making them useful for **early detection**.
* Random Forest shows the best overall balance.
* Future improvements:

  * Feature engineering
  * Handling class imbalance (e.g., SMOTE)
  * Trying ensemble methods like XGBoost or LightGBM

---

## Requirements

```bash
pip install pandas numpy scikit-learn matplotlib seaborn
```

---

##  Team Nova

## ü§ù Team Nova ‚Äì Contributors

- [@devanshigit](https://github.com/devanshigit) 
- [@maithali-b](https://github.com/Maithili-Badhan)
- [@abahadur29](https://github.com/abahadur29)

